\documentclass[parskip=full]{scrartcl}
\usepackage[T1]{fontenc}    % avoid garbled Unicode text in pdf
\usepackage[utf8]{inputenc} % use utf8 file encoding for TeX sources
\usepackage[german]{babel}  % german hyphenation, quotes, etc
\usepackage{hyperref}       % detailed hyperlink/pdf configuration
\hypersetup{                % ‘texdoc hyperref‘ for options
pdftitle={PSE: Entwicklung eines relationalen Debuggers - Testbericht},%
,%
}
\usepackage{graphicx}       % provides commands for including figures
\usepackage{csquotes}       % provides \enquote{} macro for "quotes"
\usepackage[nonumberlist]{glossaries}     % provides glossary commands
\usepackage{enumitem}
\usepackage{xcolor}
\newcommand\frage[1]{\textcolor{red}{#1}}
\renewcommand{\glstextformat}[1]{\textbf{\color{blue}\em #1}}

\font\myfont=cmr12 at 20pt

\title{
	\vspace{2cm}
	\myfont 
	Praxis der Softwareentwicklung:\\ 
	Entwicklung eines relationalen Debuggers\\
}
\subtitle{
	\vspace{1cm}
	\myfont
	Testbericht
}

\author{
	\vspace{1cm} \\
	Benedikt Wagner\\
	\and 
        \vspace{1cm} \\ 
        Chiara Staudenmaier\\
        \and 
        \vspace{1cm} \\
        Etienne Brunner\\
	\and Joana Plewnia\\
	\and Pascal Zwick\\
	\and Ulla Scheler\\
	\vspace{1cm}
	\and Betreuer: Mihai Herda, Michael Kirsten
	\vspace{4cm}
}

\begin{document}
\clearpage
\maketitle
\pagenumbering{gobble}
\newpage

\tableofcontents
\newpage
\pagenumbering{arabic}

\section{Übersicht}

\begin{figure}[!h]
\centering
\includegraphics[width=0.6\textwidth]{../Plichtenheft/logo_nongi.png}
\caption{Produktlogo}
\end{figure}

\subsection{Erklären des Vorgehens}
\subsection{Welche Qualitäten wurden wie getestet?}

Benutzeroberfläche:
Verständlichkeit \\
Übersichtlichkeit \\
Erlernbarkeit \\

System:
Fehlertoleranz/Stabilität \\
Zeit-/Ressourceneffizienz \\


\section{Testen einzelner Pakete}
Das Paket \textit{AntlrParser} besteht lediglich aus von dem verwendeten Tool Antlr (siehe Entwurfsdokument Abschnitt 10.2) generierten Klassen. Da dieses Tool öffentlich bekannt und verbreitet ist, wurde es nicht explizit getestet. Die Korrektheit der von uns spezifizierten Grammatik, aus denen Antlr diese Klassen generiert, wurde im Rahmen der Tests des Paketes \textit{Interpreter} implizit getestet.
\subsection{Gesamtabdeckung}
\subsection{Interpreter}
%TODO Annahme, dass Antlr korrekt funktioniert
Im Interpreter wurden zwei Arten von Komponententests durchgeführt. Zunächst wurden die Klassen in einer unten beschriebenen, sinnvollen Reihenfolge für sich getestet. 
Hierbei wurde vor allem getestet, dass das was funktionieren soll, funktioniert.\\ %TODO bessere Formulierung
Dann wurden Tests geschrieben, die das Zusammenspiel der gesamten Komponente Interpreter testet. Hierbei wurden insbesondere die Reaktion auf Ausnahmefälle und falsche Eingaben getestet.
\paragraph{Klassentests}
Die Aufgabe des Interpreters ist das Ausführen der vom Nutzer als Quelltext gegebenen Programme. 
Da Commandklassen die Funktionsfähigkeit von Termen vorraussetzen und diese ein korrekt implementiertes Typsystem erfordern wurde dies hier in der Reihenfolge des Schreibens der Unittests berücksichtigt. 
So wurden zunächst Unittest für die Implementierungen der abstrakten Klasse \textit{TermValue} geschrieben.
Bei der Klasse \textit{Term} und ihren Implementierungen wurde die Kenntnis der Implementierung genutzt, um hier den Aufwand der Tests niedrig zu halten. Diese Klassen sind wegen des Kompositum Musters sehr schlank und nutzen vor allem die entsprechenden Methoden der \textit{TermValue} Klassen. So wurden auch die Tests schlank gehalten und hier wurde vor allem darauf geachtet, eine hohe Abdeckung nach der in %TODO
 ???????????? beschriebenen Form zu erreichen. Die Abdeckung lag so bei fast allen dieser Klassen bei 100\%. Dennoch halfen diese Tests, eine Unachtsamkeit der Implementierungsphase zu finden, in der vergessen wurde die Klassen \textit{ArrayAccessRelationalTerm} einer Implementierungsänderung anzupassen.\\
Ein weiterer zu testender Bestandteil waren die \textit{Command}-Klassen. 
Anschließend wurden die beiden \textit{Visitor}-Klassen getestet. Hierbei war wichtig, dass diese die korrekte Baumstruktur aus Commands und Termen erzeugen.
Das Zusammenspiel all dieser Komponenten wurde in den Paketweiten Tests und in den funktionalen Gesamttests überprüft.
\paragraph{Paketweite Tests}
Eine wichtige Frage, die beim Testen beantwortet werden soll, ist die Frage nach der Robustheit des Paketes gegenüber falschen oder grenzwertigen Eingaben. Erwartet ist bei etwa einem semantisch oder syntaktisch falschen Programmtext, dass eine aussagekräftige Exception an den Aufrufer gegeben wird. Hierbei unterscheiden wir verschiedene Arten von \enquote{falschen} Eingaben:
%TODO das muss noch ausformuliert und wirklich getestet werden.
\subparagraph{Syntaktisch falsche Wlang-Programme}
fehlende main, falsche methoden definition, falsche zuweisung...
\subparagraph{Semantisch falsche Wlang-Programme}
hier gibt es viel: 
fehlendes return, while(3+2), if(2.4f), falsche Arraynutzung: int[3][4] a; a[4] = 10; oder char[2.3f] a; oder char[b] a; wobei b kein int
falscher umgang mit datentypen, falsche übergabeparameter, endlosschleifen
\subparagraph{Syntaktisch falsche Watch-Expressions oder Bedingte Breakpoints}
falsche Syntax z.B. A.x = l,y
\subparagraph{Semantisch falsche Watch-Expressions oder Bedingte Breakpoints}
z.B. Variable existiert nicht, Typfehler, Programm mit der Id existiert nicht

\paragraph{Im Interpreter gefundene Fehler}
Während der Klassentests zeigten sich wenige kleine Fehler in der Umsetzung einiger Klassen. Diese waren jedoch schnell behoben, und durch das Nutzen von JUnit konnte sichergestellt werden, dass auch nach Beheben des Fehlers kein neuer Fehler dadurch entstand.\\
In einigen Methoden der Subklassen von \textit{TermValue} zeigte sich, dass Typen des Ergebnisses nicht immer korrekt waren. So wertete sich \texttt{long} + \texttt{long} zu \texttt{double} aus. Diese Art von Fehler konnte durch ändern weniger Zeilen behoben werden.\\
Weiterhin konnte in der Klasse \textit{ArrayAccessRelationalTerm} ein Fehler gefunden werden, der noch aus einer hier nicht durchgeführten Änderung einer Implementierungsentscheidung stammte. Genauer konnte ein Array mit dem Namen \texttt{a} in Programm Z unter dem Bezeichner \texttt{Z.a} nicht abgerufen werden, da die vorhandene Implementierung das Programm Z immer an 26. Stelle in der Liste der Programme vorsah. Auch dies war schnell behoben. Weitere Fehler ergaben sich bei den umfangreichen Klassentests nicht.

%TODO Fehler bei paketweiten Tests
Beim Testen der WatchExpressions auf ihre Reaktion gegenüber falscher Eingaben zeigte sich ein Fehler in der Ausnahmebehandlung im Interpreterpaket. Bei Spezifizieren einer Watch-Expression durch den mehrdeutigen Bezeichner \texttt{a} erwartet man bei Auswertung ein \texttt{"?"} als Ergebnis, da nicht klar ist, aus welchem Programm die Variable \texttt{a} stammt. Dies funktionierte auch, doch die Eingabe \texttt{a+b} resultierte im Ergebnis "~", da in Ascii "'?'+'?' = 63+63=126 = '\~'".
Durch Werfen einer Exception im entsprechenden Term und Fangen dieser in der Klasse \textit{WatchExpression} sowie \textit{ConditionalBreakpoint} konnte dieser Fehler behoben werden.

%\subsection{Belastungstests}
\subsection{Control}
\paragraph{Paketweite Tests}
Unter Verwendung von Testhelfern wurde \textit{applyConfig} in \textit{FileHandlerInteractor} getestet.
Dabei wurde die Interaktion von \textit{FileHandlerInteractor} mit Klassen aus \textit{dibugger.control} oder anderen Paketen mit dem Sollverhalten verglichen.
\paragraph{In Control gefundene Fehler}
In der Methode \textit{applyConfig} wurden Methoden aus \textit{dibugger.userinterface} oder \textit{dibugger.debuglogic} zu oft, oder gar nicht aufgerufen.
Zur Behebeung wurden Funktionsaufrufe in \textit{applyConfig} umgeordnet oder hinzugefügt.
\subsection{Filehandler}

\section{Gesondertes Testen der Benutzeroberfläche}

\subsection{Testplan und Übersicht}
Da automatisiertes Testen der Benutzeroberfläche sehr zeitaufwändig ist, wurde die Benutzeroberfläche des Dibuggers nach einem eigenen Testplan getestet. Dieser besteht zunächst aus sowohl dummem als auch intelligentem \textit{Monkey Testing}, um Schwächen oder Absturzpunkte der Benutzeroberfläche zu finden. \\
Anschließend wurde die Benutzeroberfläche von den Entwicklern hinsichtlich Nutzerfreundlichkeit betrachtet. Aufgrund der Gefahr, hier wichtige Kriterien zu übersehen, wird zusätzlich eine Benutzerstudie durchgeführt. \\
Außerdem wurden alle Funktionen der Benutzeroberfläche einzeln (z.B. jeder Button und Menüeintrag für sich) getestet, um diese Basisfunktionen dann in den Anwendungsfällen, welche im Pflichtenheft beschrieben wurden, zusammenzufassen. Die einzelnen Funktionen wurden hierbei in die Kriterien Gestaltung, Funktionalität und Performanz aufgeteilt.

\subsection{Monkey Testing}
\subsubsection{Dummes Monkey Testing}
\subsubsection{Intelligentes Monkey Testing}

\subsection{Untersuchung der Nutzerfreundlichkeit}
Zur Verbessserung der Benutzerfreundlichkeit wurde zu den \textit{ExpressionPanels} jeweils ein +-Button hinzugefügt, um das Hinzufügen von Relationalen Ausdrücken intuitiver zu gestalten.
Weitere Änderungen werden, falls nötig, nach der Benutzerstudie durchgeführt und hier dokumentiert. Außerdem wird gegebenenfalls das Hilfedokument geändert oder erweitert.

\subsection{Test der Basisfunktionen}
\subsubsection{Gestaltung}
\paragraph{Getestete Funktionen}
In diesem Bereich wurde die Anpassung des Produkts an die vom Benutzer gewählte Fenstergröße getestet. Außerdem wurden sämtliche PopUps auf ausreichende Größe überprüft.
Weitere Basisfunktionen im Bereich Gestaltung sind das korrekte Anzeigen der Scrollbalken, der hinzugefügten ProgramPanels und des Texts in den Textboxen.
\paragraph{Vorgenommene Änderungen}
Das rechte Control-Panel, welches die ExpressionPanels und das CommandPanel enthält, bleibt nun bei einer festen Größe, um keinen Platz zu verschwenden. Bei Vergößern oder Verkleinern passt sich der Editor der ProgramPanels an die Größe des Fensters an.
\subsubsection{Funktionalität}
\paragraph{Getestete Funktionen}
\subparagraph{Menüs}
Es wurde getestet, ob die Menüeinträge und deren ActionListener valide sind, also ob die richtigen Optionen (zB verschiedene Sprachdateien) verfügbar sind, und richtig angewählt werden können.  \\
Zusätzlich wurden die Sprachdateien auf Vollständigkeit überprüft, und anschließend ob das Ändern der Sprache jederzeit möglich ist. \\
Die PopUps für Vorschläge wurden auf korrekte Funktionalität und Sprache überprüft, sowie die Menüeinträge für Vorschläge auf korrektes Aufrufen der jeweiligen PopUps. \\
Es wurde sichergestellt, dass vorgenommene Einstellungen im Einstellungsmenü nach Neustart des Produkts gleichbleiben.
\subparagraph{ProgramPanel}
Hinzufügen und Löschen bzw Zurücksetzen von ProgramPanels wurde in verschiedenen Reihenfolgen überprüft, um zusätzlich zu testen ob die Programmbezeichner korrekt berechnet werden. \\
Die Textfelder für Eingabevariablen und Schrittgröße wurden auf korrektes Weitergeben ihrer Daten geprüft. \\
Im Editorfeld, welches das jeweilige Programm enthält, waren folgende Funktionen zu testen:
\begin{itemize}
\item Hinzufügen, Löschen und Anzeige von (aktiven) Breakpoints an Zeilen im Text
\item Korrekte Zeilennummerierung bei Änderungen im Code
\item Markierbarkeit, Kopierbarkeit, Anzeige und Tab-Funktionalität im Text
\end{itemize}
Außerdem wurde die korrekte Returnvalue und das Anzeigen, Aus-und Einblenden von Variablen im Variableninspektor überprüft.
\subparagraph{Rechtes Control-Panel}
Das \textit{CommandPanel} wurde auf korrektes Ausgrauen der Buttons und richtige Übergabe von Benutzereingaben getestet.
Folgende Funktionalitäten wurden in den \textit{ExpressionPanels} überprüft:
\begin{itemize}
\item Anzeige der Auswertung
\item Übergabe der Expression
\item Hinzufügen und Löschen von Expressions
\item PopUps für Bereichsbindung
\end{itemize}
\paragraph{Vorgenommene Änderungen}
\subsubsection{Performanz}
\paragraph{Getestete Funktionen}
Während sämtlichen Tests der Benutzeroberfläche wurde darauf geachtet, ob es spürbare und störende Zeitverzögerungen bei Benutzung des Produkts gibt. Dies war nicht der Fall, jedoch kann sich in den Tests des Gesamtprodukts noch herausstellen, dass sich das Generieren des Traces, bei Betätigung des \textit{Debugmodus starten}-Buttons, unter ungünstigen Bedingungen verzögern kann.
\subsection{Testszenarien}
\paragraph{/AF10/ Hinzufügen von Programmen}
\paragraph{/AF20/ Ändern von Programmen}
\paragraph{/AF30/ Setzen von Breakpoints}
\paragraph{/AF40/ Hinzufügen von Watch-Expressions}
\paragraph{/AF50/ Codes debuggen}
\paragraph{/AF60/ Speichern eines Durchlaufs}



\section{Testen des Gesamtprodukts}

\subsection{Funktionale Tests}
\begin{itemize}
\item Einige übliche Algorithmen
\end{itemize}


\subsection{Belastungstests}

\begin{itemize}
\item Tiefe Rekursion (Ackermann)
\item Endlosschleifen
\item falsche Nutzereingaben
\item keine Nutzereingaben
\item n-viele Programme gleichzeitig debuggen
\item leere Fenster
\item falsche Dateien laden
\item Testen von Programmen aus den anderen Relationalen Programmen

\end{itemize}

\subsection{Test auf verschiedener Hard- und Software}

\subsubsection{Software}
Das Produkt wurde sowohl auf Windows (Vista SP2, 7, 8.x, 10), als auch auf Mac OS X (10.8.3+, 10.9+) und Linux (15.10 Kernel 4.2 oder höher) (64bit empfohlen) getestet.

\subsubsection{Hardware}
Das Produkt wurde auf Rechnern mit folgender Hardware getestet: \\ \\
\begin{tabular}{l||c|c|c}
   	& Betriebssystem & Prozessor & RAM \\
	\hline
	\hline
	Rechner 1 & Windows 7 64bit & AMD Phenom II X4 B50 4K 4T @3.5Ghz & 10GB DDR3 \\
	Rechner 2 & Windows 7 64bit & Intel Core i3-380M 2K 4T @2.5Ghz & 8GB DDR3 \\
	 & KDE Neon 5.11.3 64bit &  \\
	Rechner 3 & Ubuntu 17.04 64bit & Intel Core i5-7200U @3.1Ghz & 8GB DDR3 \\
	Rechner 4 & Windows 10 Pro 64bit & Intel Core i7-7500U @2.7Ghz & 16GB DDR4\\
		& Ubuntu 17.04 64bit\\
\end{tabular}


\subsection{Untersuchung von Laufzeit und Speicherverbrauch}


\section{Vorgenommene Veränderungen}

\section{Anhang}
\subsection{Testprotokoll für die Nutzerstudie}
%Testprotokoll
%getestete Programme

\end{document}
